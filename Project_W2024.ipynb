{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transcribing the videos\n",
    " ([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to implement your solution to transcribe the videos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install moviepy SpeechRecognition mutagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = 'CUR-TF-200-ACMNLP-1/video/'\n",
    "bucket_name = 'your-bucket-name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import io\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "import speech_recognition as sr\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_presigned_urls(s3_client, bucket_name, prefix, external_bucket=False):\n",
    "\n",
    "    response = s3_client.list_objects_v2(Bucket = bucket_name, Prefix=prefix)\n",
    "    presigned_urls = []\n",
    "    obj_keys = []\n",
    "\n",
    "    for item in response.get('Contents', []):\n",
    "        file_key = item['Key']\n",
    "        presigned_url = s3_client.generate_presigned_url(\n",
    "            ClientMethod='get_object',\n",
    "            Params={'Bucket': bucket_name, 'Key': file_key},\n",
    "            ExpiresIn=3600\n",
    "            )\n",
    "        presigned_urls.append(presigned_url)\n",
    "        obj_keys.append(file_key)\n",
    "    # obj_keys.pop(0)\n",
    "    # presigned_urls.pop(0)\n",
    "    return [presigned_urls, obj_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVERTING MP4 TO WAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "presigned_urls_mp4, obj_keys_mp4 = generate_presigned_urls(s3_client, bucket_name, prefix)\n",
    "print(presigned_urls_mp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mp4_to_wav(url, obj_key):\n",
    "    obj_key = obj_key.replace(' ', '_')\n",
    "    obj_key = obj_key.split('/')\n",
    "    # print(obj_key)\n",
    "    output_filename = f'audio/{obj_key[-1][:-4]}.wav'\n",
    "    # print(output_filename)\n",
    "    video_clip = VideoFileClip(url)\n",
    "    video_clip.audio.write_audiofile(output_filename)\n",
    "    print(f\"Successfully converted {obj_key} to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj_count = 0\n",
    "for url in presigned_urls_mp4:\n",
    "    mp4_to_wav(url, obj_keys_mp4[obj_count])\n",
    "    obj_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mp4_to_wav(presigned_urls_mp4[0], obj_keys_mp4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff960bf706ab4243b1d7940befb9f4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9732f3391b8f42ed9482f288b40e8093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce722f18a7154521bdaf999e68b488fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/4.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93369404a0ad4eb590b38551faa4fe78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5231e4dd0634b21b9cda56937acc4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc00911ce128437a949ff189c58ffcbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4fffb0ac8048f6a892edf7c0c3909a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e13e07e7c0a4b3580d6ad8f2214cd3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eff4a3dc6244114bc2fd7b0bd448888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22a99c3ac844594b5a60ee5f2f15fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259466ea50ad4113b1895f51a260bf18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "whisper = pipeline(\"automatic-speech-recognition\", \"openai/whisper-large-v2\", device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transcribe_audio_folder(audio_folder_path=\"audio\", output_folder_path=\"subtitles\"):\n",
    "\n",
    "    os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "    audio_files = [os.path.join(audio_folder_path, f) for f in os.listdir(audio_folder_path) if f.endswith((\".mp3\", \".wav\"))]\n",
    "\n",
    "    for audio_file in audio_files:\n",
    "        filename, _ = os.path.splitext(os.path.basename(audio_file))\n",
    "        output_filename = os.path.join(output_folder_path, f\"{filename}.txt\")\n",
    "\n",
    "        try:\n",
    "            transcription = whisper(audio_file, chunk_length_s=30)\n",
    "\n",
    "            with open(output_filename, \"w\") as transcript_file:\n",
    "                transcript_file.write(transcription[\"text\"])\n",
    "            print(f\"Transcribed: {audio_file} to {output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error transcribing {audio_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed: audio/Mod02_Sect02.wav to subtitles/Mod02_Sect02.txt\n",
      "Transcribed: audio/Mod03_Sect04_part1.wav to subtitles/Mod03_Sect04_part1.txt\n",
      "Transcribed: audio/Mod06_WrapUp.wav to subtitles/Mod06_WrapUp.txt\n",
      "Transcribed: audio/Mod06_Sect01.wav to subtitles/Mod06_Sect01.txt\n",
      "Transcribed: audio/Mod03_Sect02_part3.wav to subtitles/Mod03_Sect02_part3.txt\n",
      "Transcribed: audio/Mod03_Sect03_part1.wav to subtitles/Mod03_Sect03_part1.txt\n",
      "Transcribed: audio/Mod03_Sect03_part2.wav to subtitles/Mod03_Sect03_part2.txt\n",
      "Transcribed: audio/Mod05_Sect03_part2.wav to subtitles/Mod05_Sect03_part2.txt\n",
      "Transcribed: audio/Mod01_Course_Overview.wav to subtitles/Mod01_Course_Overview.txt\n",
      "Transcribed: audio/Mod02_Sect04.wav to subtitles/Mod02_Sect04.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 531, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_12532/1785702126.py\", line 1, in <module>\n",
      "    transcribe_audio_folder()\n",
      "  File \"/tmp/ipykernel_12532/1984879712.py\", line 25, in transcribe_audio_folder\n",
      "    transcription = whisper(audio_file, chunk_length_s=30)  # Adjust chunk_length as needed\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 285, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n",
      "    logger.warning_once(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\n",
      "Arguments: (<class 'UserWarning'>,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed: audio/Mod04_WrapUp.wav to subtitles/Mod04_WrapUp.txt\n",
      "Transcribed: audio/Mod02_Sect05.wav to subtitles/Mod02_Sect05.txt\n",
      "Transcribed: audio/Mod03_Sect08.wav to subtitles/Mod03_Sect08.txt\n",
      "Transcribed: audio/Mod04_Sect02_part1.wav to subtitles/Mod04_Sect02_part1.txt\n",
      "Transcribed: audio/Mod05_Sect02_part1_ver2.wav to subtitles/Mod05_Sect02_part1_ver2.txt\n",
      "Transcribed: audio/Mod07_Sect01.wav to subtitles/Mod07_Sect01.txt\n",
      "Transcribed: audio/Mod03_Sect02_part1.wav to subtitles/Mod03_Sect02_part1.txt\n",
      "Transcribed: audio/Mod06_Sect02.wav to subtitles/Mod06_Sect02.txt\n",
      "Transcribed: audio/Mod05_Sect03_part4_ver2.wav to subtitles/Mod05_Sect03_part4_ver2.txt\n",
      "Transcribed: audio/Mod02_Intro.wav to subtitles/Mod02_Intro.txt\n",
      "Transcribed: audio/Mod03_Sect06.wav to subtitles/Mod03_Sect06.txt\n",
      "Error transcribing audio/.wav: Soundfile is either not in the correct format or is malformed. Ensure that the soundfile has a valid audio file extension (e.g. wav, flac or mp3) and is not corrupted. If reading from a remote URL, ensure that the URL is the full address to **download** the audio file.\n",
      "Transcribed: audio/Mod05_Sect03_part3.wav to subtitles/Mod05_Sect03_part3.txt\n",
      "Transcribed: audio/Mod03_Sect02_part2.wav to subtitles/Mod03_Sect02_part2.txt\n",
      "Transcribed: audio/Mod03_Sect01.wav to subtitles/Mod03_Sect01.txt\n",
      "Transcribed: audio/Mod03_Sect07_part3.wav to subtitles/Mod03_Sect07_part3.txt\n",
      "Transcribed: audio/Mod03_Sect05.wav to subtitles/Mod03_Sect05.txt\n",
      "Transcribed: audio/Mod03_Sect07_part1.wav to subtitles/Mod03_Sect07_part1.txt\n",
      "Transcribed: audio/Mod04_Sect02_part3.wav to subtitles/Mod04_Sect02_part3.txt\n",
      "Transcribed: audio/Mod02_Sect01.wav to subtitles/Mod02_Sect01.txt\n",
      "Transcribed: audio/Mod03_Sect03_part3.wav to subtitles/Mod03_Sect03_part3.txt\n",
      "Transcribed: audio/Mod03_WrapUp.wav to subtitles/Mod03_WrapUp.txt\n",
      "Transcribed: audio/Mod05_Intro.wav to subtitles/Mod05_Intro.txt\n",
      "Transcribed: audio/Mod04_Sect01.wav to subtitles/Mod04_Sect01.txt\n",
      "Transcribed: audio/Mod03_Intro.wav to subtitles/Mod03_Intro.txt\n",
      "Transcribed: audio/Mod02_WrapUp.wav to subtitles/Mod02_WrapUp.txt\n",
      "Transcribed: audio/Mod05_Sect01_ver2.wav to subtitles/Mod05_Sect01_ver2.txt\n",
      "Transcribed: audio/Mod05_Sect03_part1.wav to subtitles/Mod05_Sect03_part1.txt\n",
      "Transcribed: audio/Mod03_Sect04_part3.wav to subtitles/Mod03_Sect04_part3.txt\n",
      "Transcribed: audio/Mod06_Intro.wav to subtitles/Mod06_Intro.txt\n",
      "Transcribed: audio/Mod05_Sect02_part2.wav to subtitles/Mod05_Sect02_part2.txt\n",
      "Transcribed: audio/Mod05_WrapUp_ver2.wav to subtitles/Mod05_WrapUp_ver2.txt\n",
      "Transcribed: audio/Mod03_Sect07_part2.wav to subtitles/Mod03_Sect07_part2.txt\n",
      "Transcribed: audio/Mod02_Sect03.wav to subtitles/Mod02_Sect03.txt\n",
      "Transcribed: audio/Mod04_Intro.wav to subtitles/Mod04_Intro.txt\n",
      "Transcribed: audio/Mod04_Sect02_part2.wav to subtitles/Mod04_Sect02_part2.txt\n",
      "Transcribed: audio/Mod03_Sect04_part2.wav to subtitles/Mod03_Sect04_part2.txt\n"
     ]
    }
   ],
   "source": [
    "transcribe_audio_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normalizing the text\n",
    "([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to perform any text normalization steps that are necessary for your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install nltk numpy spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text_file_path):\n",
    "\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "    with open(text_file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    normalized_text = text.lower()\n",
    "    normalized_text = ''.join([c for c in normalized_text if c.isalnum() or c.isspace()])\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    normalized_text = ' '.join([word for word in normalized_text.split() if word not in stopwords])\n",
    "\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extracting key phrases and topics\n",
    "([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to extract the key phrases and topics from the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tf_idf(text):\n",
    "\n",
    "    word_counts = FreqDist(text.split())\n",
    "\n",
    "    total_words = len(text.split())\n",
    "\n",
    "    documents = [text]\n",
    "\n",
    "    tf_idf_scores = {}\n",
    "\n",
    "    for word, count in word_counts.items():\n",
    "        tf = count / total_words \n",
    "        df = 0\n",
    "\n",
    "        for doc in documents:\n",
    "            if word in doc:\n",
    "                df += 1\n",
    "\n",
    "        idf = np.log((len(documents) / (df + 1)))\n",
    "\n",
    "        tf_idf_scores[word] = tf * idf\n",
    "\n",
    "    return tf_idf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_key_phrases(text):\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm')  # Load spaCy model\n",
    "    doc = nlp(text)\n",
    "    phrases = []\n",
    "    for noun_phrase in doc.noun_chunks:\n",
    "        phrases.append(noun_phrase.text)\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_subtitles(subtitles_folder=\"subtitles\"):\n",
    "    all_results = {}\n",
    "    for filename in os.listdir(subtitles_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(subtitles_folder, filename)\n",
    "            normalized_text = preprocess_text(file_path)\n",
    "            tf_idf_scores = tf_idf(normalized_text)\n",
    "            top_10_keywords = FreqDist(tf_idf_scores).most_common(10)  \n",
    "            key_phrases = extract_key_phrases(normalized_text)\n",
    "\n",
    "            all_results[filename] = {\n",
    "                  \"top_keywords\": top_10_keywords,\n",
    "                  \"key_phrases\": key_phrases\n",
    "              }\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_analysis_results = analyze_subtitles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def search_videos(all_results, search_query):\n",
    "\n",
    "    matching_videos = []\n",
    "    query_keywords = [word.strip() for word in search_query.split(\",\") if word.strip()]\n",
    "    query_phrases = [phrase.strip() for phrase in search_query.split(\",\") if phrase.strip()]\n",
    "\n",
    "    for filename, video_data in all_results.items():\n",
    "\n",
    "        top_keywords = [word for word, _ in video_data[\"top_keywords\"]]\n",
    "        key_phrases = video_data[\"key_phrases\"]\n",
    "\n",
    "        keyword_match = any(keyword in top_keywords for keyword in query_keywords)\n",
    "\n",
    "        phrase_match = any(phrase in key_phrases for phrase in query_phrases)\n",
    "\n",
    "        if keyword_match or phrase_match:\n",
    "            matching_videos.append(filename)\n",
    "\n",
    "    return matching_videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating the dashboard\n",
    "([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to create the dashboard for your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_client=boto3.client('s3')\n",
    "bucket_name = 'your-bucket-name'\n",
    "prefix = 'CUR-TF-200-ACMNLP-1/video/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_urls(s3_client, bucket_name, prefix, key):\n",
    "    response = s3_client.list_objects_v2(Bucket = bucket_name, Prefix=prefix)\n",
    "    presigned_urls = []\n",
    "    obj_keys = []\n",
    "    key = key[:-4]\n",
    "    for item in response.get('Contents', []):\n",
    "        file_key = item['Key']\n",
    "        modified_key = file_key.replace(' ', '_')\n",
    "        if key in modified_key:\n",
    "            presigned_url = s3_client.generate_presigned_url(\n",
    "                ClientMethod='get_object',\n",
    "                Params={'Bucket': bucket_name, 'Key': file_key},\n",
    "                ExpiresIn=3600  \n",
    "                )\n",
    "            presigned_urls.append(presigned_url)\n",
    "            file_key = file_key.split('/')[-1][:-4]\n",
    "            obj_keys.append(file_key)\n",
    "\n",
    "    return [presigned_urls, obj_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, Layout, Textarea, Button, Label, VBox, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_video_list = list(all_analysis_results.keys())\n",
    "video_list_sorted = initial_video_list.copy()\n",
    "video_list_sorted.sort()\n",
    "\n",
    "search_bar = Textarea(description=\"Search Keywords or Phrases (separated by commas)\", value=\"\")\n",
    "\n",
    "search_button = Button(description=\"Search\")\n",
    "\n",
    "video_list_output = HTML(layout={'width': '100%'})\n",
    "\n",
    "links_html = ''\n",
    "for key in video_list_sorted:\n",
    "    video_url, video_name = get_urls(s3_client, bucket_name, prefix, key)\n",
    "    link_html = f'<h3><a href=\\\"{video_url[0]}\\\"  target=\\'blank\\'>{video_name[0]}</a></h3><br>'\n",
    "    links_html += link_html\n",
    "video_list_output.value = links_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da02e57f51a4216bfea2df824f2db7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='', description='Search Keywords or Phrases (separated by commas)'), Button(descâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def update_video_list(search_query):\n",
    "\n",
    "    matching_videos = search_videos(all_analysis_results, search_bar.value)\n",
    "    matching_videos_sorted = matching_videos.copy()\n",
    "    matching_videos_sorted.sort()\n",
    "    links_html = \"\"\n",
    "    for key in matching_videos_sorted:\n",
    "        video_url, video_name = get_urls(s3_client, bucket_name, prefix, key)\n",
    "        link_html = f'<h3><a href=\\\"{video_url[0]}\\\" target=\\'blank\\'>{video_name[0]}</a></h3><br>'\n",
    "        links_html += link_html\n",
    "    video_list_output.value = links_html\n",
    "\n",
    "\n",
    "search_button.on_click(update_video_list)\n",
    "\n",
    "\n",
    "layout = VBox([search_bar, search_button, video_list_output])\n",
    "\n",
    "display(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b71a13339a0be9489ff337af97259fe0ed71e682663adc836bae31ac651d564e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
